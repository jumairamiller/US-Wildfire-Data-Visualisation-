{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisulaizationCoursework",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumairamiller/US-Wildfire-Data-Visualisation-/blob/main/VisulaizationCoursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnFAcviOmyLN"
      },
      "source": [
        "# To get authorization code, follow the link and log into gmail account:\n",
        "\n",
        "username - **jummyller.m27@gmail.com**\n",
        "\n",
        "password - **Arch@mb@ultM27**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngddsT26gvN9"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEXHa-xQ6N54"
      },
      "source": [
        "# required imports\n",
        "import numpy as np   \n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from altair import datum, expr\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import math\n",
        "from math import e\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "53HiKV456N-M",
        "outputId": "735d366a-b228-4f01-c41a-83a6a2419214"
      },
      "source": [
        "#Load Data\n",
        "states_data = pd.read_csv('drive/My Drive/states.csv')\n",
        "df=pd.read_csv('drive/My Drive/firesData.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0335e3f66176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstates_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/states.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/firesData.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/states.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cWW10AT6hU8"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9952P3E9djeg"
      },
      "source": [
        "\"\"\"\n",
        "Note to the dev. The variable \"Decade\" was chosen haphazardly and represents\n",
        "a now-defunct design choice. A total refactor to instead use TIME_BIN or\n",
        "TIME_PERIOD would aid readability.\n",
        "\"\"\"\n",
        "\n",
        "def to_decade(year):\n",
        "  if year <= 1999:\n",
        "    return \"1993-1999\"\n",
        "  elif year <= 2004:\n",
        "    return \"2000-2004\"\n",
        "  elif year <= 2009:\n",
        "    return \"2005-2009\"\n",
        "  elif year <= 2015:\n",
        "    return \"2010-2015\"\n",
        "  return None\n",
        "\n",
        "# Create the Decade column for time series binning\n",
        "df[\"DECADE\"] = df[\"FIRE_YEAR\"].apply(to_decade)\n",
        "df[\"DECADE_NUMBER\"] = df[\"DECADE\"].apply(lambda d: [\"1993-1999\", \"2000-2004\", \"2005-2009\", \"2010-2015\"].index(d))\n",
        "\n",
        "df.sample(n=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5GXFGQGunP6"
      },
      "source": [
        "# Frequency of fire, by state and decade\n",
        "df[\"STATE_DECADE_FREQUENCY\"] = df.groupby([\"DECADE\", \"STATE\"])[\"STATE\"].transform(pd.Series.count)\n",
        "# Precalculate the sum of fire size by state and decade\n",
        "df[\"STATE_DECADE_FIRE_SIZE_SUM\"] = df.groupby([\"DECADE\", \"STATE\"])[\"FIRE_SIZE\"].transform(pd.Series.sum)\n",
        "\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AquMIyRNgXxU"
      },
      "source": [
        "\"\"\"\n",
        "Note: The copious use of df.apply(...) makes this section run particularly slowly\n",
        "especially on the full datatset.\n",
        "Less heavy data transformation could be used to reduce this overhead.\n",
        "\"\"\"\n",
        "\n",
        "def grade_fire_size(fire_size):\n",
        "  \"\"\"\n",
        "  Takes fire size (in acres) and returns the fire size designation as an index.\n",
        "  These values are taken from https://www.nwcg.gov/data-standards/approved/fire-size-class\n",
        "  As the dataset tracks all fires >= 5000 as a G we cutoff at this place too\n",
        "  These can be converted to letters alphabetically\n",
        "  \"\"\"\n",
        "  size_cutoffs = [0.26, 10, 100, 300, 1000, 5000]\n",
        "  indicies = [1, 2, 3, 4, 5, 6]\n",
        "  for index, limit in zip(indicies, size_cutoffs):\n",
        "    if fire_size < limit:\n",
        "      return index\n",
        "  # Code G is for all fires larger than 5000 acres\n",
        "  return 7\n",
        "\n",
        "def weight_function(x):\n",
        "  \"\"\"\n",
        "  How the weight of a value should be decided\n",
        "  params:\n",
        "    x: ndarray or df projection\n",
        "  \"\"\"\n",
        "  return x\n",
        "\n",
        "# Precalc weights\n",
        "df[\"WEIGHTS\"] = weight_function(df[\"FIRE_SIZE\"])\n",
        "\n",
        "aggs = df.groupby([\"STATE\", \"DECADE\"])[\"FIRE_SIZE\"].agg(\n",
        "    MIN_FIRE_SIZE_STATE_DECADE=np.min,\n",
        "    MAX_FIRE_SIZE_STATE_DECADE=np.max,\n",
        "    MEAN_FIRE_SIZE_STATE_DECADE=np.mean).reset_index()\n",
        "aggs = pd.merge(aggs, df.groupby([\"STATE\", \"DECADE\"]).apply(lambda x: np.average(x['FIRE_SIZE'])).reset_index(name=\"WAV_FIRE_SIZE_STATE_DECADE\"), on=['STATE', 'DECADE'])\n",
        "\n",
        "df = pd.merge(df, aggs, on=['STATE', 'DECADE'])\n",
        "\n",
        "df[\"CATAGORY\"] = df.apply(lambda row: grade_fire_size(row.WAV_FIRE_SIZE_STATE_DECADE), axis = 1)\n",
        "df[\"CATAGORY_LETTER\"] = df.apply(lambda row: chr(ord('A') + (row.CATAGORY - 1)), axis = 1)\n",
        "df[\"FIRE_SIZE_CLASS\"].value_counts().sort_index().plot.bar()\n",
        "\n",
        "# For distribution mapping purposes\n",
        "plt.show()\n",
        "df[\"CATAGORY\"].value_counts().sort_index().plot.bar()\n",
        "plt.show()\n",
        "\n",
        "x_max = 10\n",
        "plt.plot(np.linspace(0, x_max, 10000), weight_function(np.linspace(0, x_max, 10000)))\n",
        "plt.plot([0, x_max], [1, 1], color=\"red\")\n",
        "plt.show()\n",
        "\n",
        "df.sample(n=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ7cUwidHttc"
      },
      "source": [
        "# Linear Regression model shorthand\n",
        "\n",
        "def train_model(Xs, ys):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.3, random_state=0)\n",
        "  model = LinearRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  return model, r2_score(y_test, y_pred)\n",
        "\n",
        "def plot_model(Xs, ys, model):\n",
        "  plt.scatter(Xs, ys)\n",
        "  plt.plot((min(Xs), max(Xs)), (model.predict([[min(Xs)]])[0][0], model.predict([[max(Xs)]])[0][0]), 'red')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVM0ZpPvLRQJ"
      },
      "source": [
        "state_models = {}\n",
        "for state in df[\"STATE\"].unique():\n",
        "  state_proj = df[df[\"STATE\"] == state]\n",
        "  if(len(state_proj) <= 2):\n",
        "    continue\n",
        "  state_models[state] = {}\n",
        "  freqs = state_proj.groupby([\"FIRE_YEAR\"])[\"FIRE_YEAR\"].count().reset_index(name=\"FREQUENCY\")\n",
        "  # Add the training data to each model\n",
        "  state_models[state][\"Fire Size\"] = train_model(state_proj[\"FIRE_YEAR\"].values.reshape(-1,1), state_proj[\"FIRE_SIZE\"].values.reshape(-1,1))\n",
        "  state_models[state][\"Fire Freq\"] = train_model(freqs[\"FIRE_YEAR\"].values.reshape(-1,1), freqs[\"FREQUENCY\"].values.reshape(-1,1))\n",
        "  state_models[state][\"Fire Catagory\"] = train_model(state_proj[\"DECADE_NUMBER\"].values.reshape(-1,1), state_proj[\"CATAGORY\"].values.reshape(-1,1))\n",
        "print(state_models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrHR9PXGz6C6"
      },
      "source": [
        "# Show the states in state data on a chart, for debugging purposes\n",
        "alt.Chart(states_data).mark_circle(size=60).encode(\n",
        "    x='LON',\n",
        "    y='LAT',\n",
        "    tooltip=[\"STATE\"]\n",
        ").interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91tMmxf6ajSM"
      },
      "source": [
        "\"\"\"\n",
        "Code for circle repulsion\n",
        "\"\"\"\n",
        "class Circle:\n",
        "\tdef __init__(self, x, y, r, c = '#000000'):\n",
        "\t\tself.xy = np.array([x,y])\n",
        "\t\tself.r = r\n",
        "\t\tself.c = c\n",
        "\n",
        "\t@property\n",
        "\tdef x(self):\n",
        "\t\treturn self.xy[0]\n",
        "\n",
        "\t@property\n",
        "\tdef y(self):\n",
        "\t\treturn self.xy[1]\n",
        "\n",
        "\tdef sqd_distance(self, other):\n",
        "\t\tdelta_x = other.x - self.x\n",
        "\t\tdelta_y = other.y - self.y\n",
        "\t\treturn delta_x * delta_x + delta_y * delta_y\n",
        "\n",
        "\tdef distance(self, other):\n",
        "\t\treturn math.sqrt(self.sqd_distance(other))\n",
        "\n",
        "\tdef direction(self, other):\n",
        "\t\treturn other.xy - self.xy\n",
        "\n",
        "\tdef intersects(self, other):\n",
        "\t\tr_comb = self.r + other.r\n",
        "\t\t# Bounding box test\n",
        "\t\tif self.x - r_comb < other.x < self.x + r_comb and self.y - r_comb < other.y < self.y + r_comb:\n",
        "\t\t\treturn self.sqd_distance(other) < r_comb ** 2\n",
        "\t\treturn False\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef random_circle(max_x, max_y, max_r):\n",
        "\t\tx = np.random.uniform(0, max_x)\n",
        "\t\ty = np.random.uniform(0, max_y)\n",
        "\t\tc = '#%02x%02x%02x' % (int(255*x/max_x), 0, int(255*y/max_y))\n",
        "\t\treturn Circle(np.random.uniform(0, max_x), np.random.uniform(0, max_y), np.random.uniform(0, max_r), c=c)\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn \"CIRCLE ({0}, {1}) r={2}\".format(self.x, self.y, self.r)\n",
        "\n",
        "def find_center(circles):\n",
        "\txs = [c.x for c in circles]\n",
        "\tys = [c.y for c in circles]\n",
        "\treturn np.mean(xs), np.mean(ys)\n",
        "\n",
        "def circle_repel(circles):\n",
        "\t# Sort circles by distance from the center (assume that we wish to handle the furthest from center first\n",
        "\tmidpoint = np.array(find_center(circles))\n",
        "\tmidpoint_circle = Circle(midpoint[0], midpoint[1], 0)\n",
        "\tdone = False\n",
        "\ttolerence = 0.25\n",
        "\t# Separate circles out\n",
        "\twhile not done:\n",
        "\t\tdone = True\n",
        "\t\tsorted_circles = sorted(circles, reverse=True, key=lambda c: midpoint_circle.sqd_distance(c))\n",
        "\n",
        "\t\tfor i in range(len(sorted_circles) - 1):\n",
        "\t\t\tcircle = sorted_circles[i]\n",
        "\t\t\tfor j in range(i+1, len(sorted_circles)):\n",
        "\t\t\t\tother_circle = sorted_circles[j]\n",
        "\t\t\t\tif circle != other_circle and circle.intersects(other_circle):\n",
        "\t\t\t\t\t# In the case that the two circles lie on the same point exactly\n",
        "\t\t\t\t\tif all(circle.xy == other_circle.xy):\n",
        "\t\t\t\t\t\t# In the case that this is also the exact midpoint\n",
        "\t\t\t\t\t\tif all(circle.xy == midpoint):\n",
        "\t\t\t\t\t\t\ttravel_vector = np.random.random(2)\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\ttravel_vector = circle.xy - midpoint\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\ttravel_vector = other_circle.direction(circle)\n",
        "\n",
        "\t\t\t\t\ttravel_vector = travel_vector / np.linalg.norm(travel_vector)\n",
        "\t\t\t\t\t# By how much do the circles need to move\n",
        "\t\t\t\t\toverlap = ((circle.r + other_circle.r) - circle.distance(other_circle)) + tolerence\n",
        "\n",
        "\t\t\t\t\tratio = other_circle.r / (circle.r + other_circle.r)\n",
        "\t\t\t\t\tthis_move = ratio * overlap\n",
        "\t\t\t\t\tother_move = (1 - ratio) * overlap\n",
        "\t\t\t\t\tcircle.xy = circle.xy + (travel_vector * this_move)\n",
        "\t\t\t\t\tother_circle.xy = other_circle.xy - (travel_vector * other_move)\n",
        "\t\t\t\t\tdone = False\n",
        "\n",
        "\t# Collect them together\n",
        "\tsorted_circles = sorted(circles, reverse=False, key=lambda c: midpoint_circle.distance(c) - c.r)\n",
        "\tfor circle in sorted_circles:\n",
        "\t\tmidpoint_distance = midpoint_circle.distance(circle) - circle.r\n",
        "\t\tclosest_neighbour_distance = None\n",
        "\t\tclosest_neighbour = None\n",
        "\t\tfor other_circle in sorted_circles:\n",
        "\t\t\tif circle != other_circle and midpoint_circle.distance(other_circle) - circle.r <= midpoint_distance:\n",
        "\t\t\t\td = circle.distance(other_circle) - (circle.r + other_circle.r)\n",
        "\t\t\t\tif closest_neighbour is None or d < closest_neighbour_distance:\n",
        "\t\t\t\t\tclosest_neighbour = other_circle\n",
        "\t\t\t\t\tclosest_neighbour_distance = d\n",
        "\t\tif closest_neighbour is None:\n",
        "\t\t\t#handle first circle case\n",
        "\t\t\tcontinue\n",
        "\t\tif closest_neighbour_distance < (tolerence * 2):\n",
        "\t\t\t# Already borders another circle\n",
        "\t\t\tcontinue\n",
        "\t\ttravel_vector = circle.direction(closest_neighbour)\n",
        "\t\ttravel_vector = (closest_neighbour_distance * travel_vector) / np.linalg.norm(travel_vector)\n",
        "\t\tcircle.xy = circle.xy + travel_vector\n",
        "\n",
        "\n",
        "\n",
        "def plot_circles(circles, midpoint=None):\n",
        "\txs = [c.x for c in circles]\n",
        "\tys = [c.y for c in circles]\n",
        "\trs = [c.r for c in circles]\n",
        "\txs_range = max(xs) - min(xs)\n",
        "\tys_range = max(ys) - min(ys)\n",
        "\tmax_range = max(xs_range, ys_range) + (2 * max(rs))\n",
        "\txs_midpoint = min(xs) + (xs_range / 2)\n",
        "\tys_midpoint = min(ys) + (ys_range / 2)\n",
        "\n",
        "\tfigure, ax = plt.subplots()\n",
        "\tplt.xlim(xs_midpoint - max_range / 2, xs_midpoint + max_range / 2)\n",
        "\tplt.ylim(ys_midpoint - max_range / 2, ys_midpoint + max_range / 2)\n",
        "\n",
        "\tfor circle in circles:\n",
        "\t\tdraw_circle = plt.Circle((circle.x, circle.y), circle.r, fill=False, color=circle.c)\n",
        "\t\tax.add_artist(draw_circle)\n",
        "\n",
        "\tif midpoint is not None:\n",
        "\t\tplt.plot(midpoint[0], midpoint[1], 'b+')\n",
        "\n",
        "\tax.set_aspect(1)\n",
        "\tplt.show()\n",
        " \n",
        "def pack_circles(data, x_key, y_key, size_key, foreign_key=None, fit_unit_circle=True, size_normalizing_factor = (.05, .25)):\n",
        "  if foreign_key is not None:\n",
        "    circle_df = data[[x_key, y_key, size_key, foreign_key]].copy()\n",
        "  else:\n",
        "    circle_df = data[[x_key, y_key, size_key]].copy()\n",
        "\n",
        "  x_range = circle_df[x_key].max() - circle_df[x_key].min()\n",
        "  y_range = circle_df[y_key].max() - circle_df[y_key].min()\n",
        "  size_range = circle_df[size_key].max() - circle_df[size_key].min()\n",
        "  max_range = max(x_range, y_range)\n",
        "  # Set all sizes between one tenth and one quarter of the screen. This is set experimentally\n",
        "  scale = max_range * size_normalizing_factor[0], max_range * size_normalizing_factor[1]\n",
        "  if size_range == 0:\n",
        "    circle_df[size_key] = 1\n",
        "  else:\n",
        "    circle_df[size_key] = (circle_df[size_key] - circle_df[size_key].min()) / size_range\n",
        "    circle_df[size_key] *= scale[1] - scale[0]\n",
        "    circle_df[size_key] += scale[0]\n",
        "  \n",
        "  circle_data = {\n",
        "    i if foreign_key is None else row[foreign_key]: Circle(row[x_key], row[y_key], row[size_key]) for i, row in circle_df.iterrows()\n",
        "  }\n",
        "  circles = list(circle_data.values())\n",
        "  circle_repel(circles)\n",
        "  if foreign_key is None:\n",
        "    packed_data = pd.DataFrame({\n",
        "      'CIRCLE_X': [c.x for c in circles],\n",
        "      'CIRCLE_Y': [c.y for c in circles],\n",
        "      'CIRCLE_R': [c.r for c in circles]\n",
        "      })\n",
        "  else:\n",
        "    packed_data = pd.DataFrame({\n",
        "    'CIRCLE_X': [c.x for c in circles],\n",
        "    'CIRCLE_Y': [c.y for c in circles],\n",
        "    'CIRCLE_R': [c.r for c in circles],\n",
        "    foreign_key: [row[foreign_key] for _, row in circle_df.iterrows()]\n",
        "    })\n",
        "  if not fit_unit_circle:\n",
        "    return packed_data\n",
        "  # Transform the data into altair chart space. Altair, it seems,\n",
        "  # DOES NOT WANT ANYBODY plotting a circle on their charts the easy way\n",
        "  # so this is the workaround.\n",
        "  extents_x_min = (packed_data[\"CIRCLE_X\"] - packed_data[\"CIRCLE_R\"]).min()\n",
        "  extents_x_max = (packed_data[\"CIRCLE_X\"] + packed_data[\"CIRCLE_R\"]).max()\n",
        "  extents_y_min = (packed_data[\"CIRCLE_Y\"] - packed_data[\"CIRCLE_R\"]).min()\n",
        "  extents_y_max = (packed_data[\"CIRCLE_Y\"] + packed_data[\"CIRCLE_R\"]).max()\n",
        "  midpoint_x = extents_x_min + ((extents_x_max - extents_x_min) / 2)\n",
        "  midpoint_y = extents_y_min + ((extents_y_max - extents_y_min) / 2)\n",
        "  extents_max = max(extents_x_max - extents_x_min, extents_y_max - extents_y_min)\n",
        "  # Transform the data into altair space (normalized between -1 and 1)\n",
        "  packed_data[\"CIRCLE_X\"] = packed_data[\"CIRCLE_X\"] - midpoint_x\n",
        "  packed_data[\"CIRCLE_Y\"] = packed_data[\"CIRCLE_Y\"] - midpoint_y\n",
        "  packed_data[\"CIRCLE_X\"] = packed_data[\"CIRCLE_X\"] / (extents_max * 0.5)\n",
        "  packed_data[\"CIRCLE_Y\"] = packed_data[\"CIRCLE_Y\"] / (extents_max * 0.5)\n",
        "  packed_data[\"CIRCLE_R\"] = packed_data[\"CIRCLE_R\"] / (extents_max * 0.5)\n",
        "  packed_data[\"CIRCLE_A\"] = (packed_data[\"CIRCLE_R\"] * 2) ** 2\n",
        "  return packed_data\n",
        "\n",
        "# Used to test repulsion visually\n",
        "circle_data = {i: Circle(row[\"LON\"], row[\"LAT\"], np.random.uniform(8, 20)) for i, row in states_data.iterrows()}\n",
        "circles = list(circle_data.values())\n",
        "plot_circles(circles, find_center(circles))\n",
        "circle_repel(circles)\n",
        "plot_circles(circles, find_center(circles))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahxzkKmUr1u5"
      },
      "source": [
        "# Combine state and wildfire data so state names are kept.\n",
        "df = pd.merge(df,\n",
        "         states_data,\n",
        "         left_on=\"STATE\",\n",
        "         right_on=\"ABBV\",\n",
        "         suffixes=(\"_ABBV\", None),\n",
        "         how='left')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bmIGMVBAOug"
      },
      "source": [
        "## Aggregate Dataframe\n",
        "working_data = df.groupby([\"DECADE\", \"STATE\"]).first().reset_index()\n",
        "working_data = working_data.drop([\"OBJECTID\", 'Unnamed: 0', 'OBJECTID', 'FOD_ID', 'FPA_ID', 'SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'SOURCE_REPORTING_UNIT', 'SOURCE_REPORTING_UNIT_NAME', 'LOCAL_FIRE_REPORT_ID',\n",
        "       'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME', 'ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME', 'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'STAT_CAUSE_CODE', 'CONT_DATE', 'CONT_DOY', 'CONT_TIME', 'LATITUDE',\n",
        "       'LONGITUDE', 'OWNER_CODE', 'OWNER_DESCR', 'COUNTY', 'FIPS_CODE', 'FIPS_NAME', 'Shape', 'WEIGHTS', \"STAT_CAUSE_DESCR\", \"FIRE_SIZE\", \"FIRE_SIZE_CLASS\", \"FIRE_YEAR\"], axis=1)\n",
        "for state in working_data[\"ABBV\"].unique():\n",
        "  if state not in state_models.keys():\n",
        "    continue\n",
        "  size_model, _ = state_models[state][\"Fire Size\"]\n",
        "  freq_model, _ = state_models[state][\"Fire Freq\"]\n",
        "  cat_model, _ = state_models[state][\"Fire Catagory\"]\n",
        "  old_row = working_data.loc[working_data['ABBV'] == state]\n",
        "  catagory = int(round(min(max(cat_model.predict([[2021]])[0][0], 1), 7)))\n",
        "  freq =  max(0, freq_model.predict([[2021]])[0][0])\n",
        "  if freq == 0:\n",
        "    continue\n",
        "  # Add synthetic data\n",
        "  new_row = pd.DataFrame({\n",
        "      \"DECADE\": [\"2021 Predictions\"],\n",
        "      \"STATE\": [old_row[\"STATE\"].iloc[0]],\n",
        "      \"STATE_ABBV\": [old_row[\"STATE_ABBV\"].iloc[0]],\n",
        "      \"DECADE_NUMBER\": [-1],\n",
        "      \"STATE_DECADE_FREQUENCY\":[ freq],\n",
        "      \"STATE_DECADE_FIRE_SIZE_SUM\": [None],\n",
        "      \"MIN_FIRE_SIZE_STATE_DECADE\": [None],\n",
        "      \"MAX_FIRE_SIZE_STATE_DECADE\": [None],\n",
        "      \"MEAN_FIRE_SIZE_STATE_DECADE\": [None],\n",
        "      \"WAV_FIRE_SIZE_STATE_DECADE\": [None],\n",
        "      \"CATAGORY\": [catagory],\n",
        "      \"CATAGORY_LETTER\": [chr(ord('A') + (catagory - 1))],\n",
        "      \"ABBV\": [old_row[\"ABBV\"].iloc[0]],\n",
        "      \"LAT\": [old_row[\"LAT\"].iloc[0]],\n",
        "      \"LON\": [old_row[\"LON\"].iloc[0]],\n",
        "      \"POP\": [None]\n",
        "  })\n",
        "  working_data = working_data.append(new_row, ignore_index=True)\n",
        "working_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJfuDWb6WLOO"
      },
      "source": [
        "acc_list = []\n",
        "for key in state_models.keys():\n",
        "  for model_key in state_models[key].keys():\n",
        "    acc_list.append(state_models[key][model_key][1])\n",
        "acc_list = sorted(acc_list)\n",
        "acc_list = acc_list[5:-5]\n",
        "plt.boxplot(acc_list)\n",
        "print(acc_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asZAEN2_y_ID"
      },
      "source": [
        "cause_counts = df.groupby([\"STATE\", \"DECADE\", \"STAT_CAUSE_DESCR\"])[\"STAT_CAUSE_DESCR\"].count().reset_index(name=\"COUNT\")\n",
        "sizes_counts = df.groupby([\"STATE\", \"DECADE\", \"FIRE_SIZE_CLASS\"])[\"FIRE_SIZE_CLASS\"].count().reset_index(name=\"COUNT\")\n",
        "sizes_counts = pd.merge(sizes_counts, df.groupby([\"STATE\", \"DECADE\"])[\"DECADE\"].count().reset_index(name=\"TOTAL\"), on=[\"STATE\", \"DECADE\"])\n",
        "sizes_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEceMK3Yf9a0"
      },
      "source": [
        "chart_size = 600\n",
        "\n",
        "def get_circle_positions_by_decade(raw_data, decade):\n",
        "  decade_data = raw_data[raw_data[\"DECADE\"] == decade].groupby([\"ABBV\"])[(\"ABBV\", \"LAT\", \"LON\", \"STATE_DECADE_FREQUENCY\")].mean()\n",
        "  decade_data = decade_data.reset_index()\n",
        "  return pack_circles(decade_data, \"LON\", \"LAT\", \"STATE_DECADE_FREQUENCY\", \"ABBV\")\n",
        "\n",
        "\n",
        "\n",
        "decades = list(working_data[\"DECADE\"].unique())\n",
        "circle_positions = None\n",
        "for decade in decades:\n",
        "  d = get_circle_positions_by_decade(working_data, decade)\n",
        "  d[\"DECADE\"] = decade\n",
        "  d[\"MAX_PIXEL_AREA\"] = (d['CIRCLE_R'].max() * chart_size) ** 2\n",
        "  if circle_positions is None:\n",
        "    circle_positions = d\n",
        "  else:\n",
        "    circle_positions = circle_positions.append(d, ignore_index=True)\n",
        "\n",
        "working_data = pd.merge(working_data, circle_positions, on=[\"ABBV\", \"DECADE\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l88wcXb8MHNA"
      },
      "source": [
        "def quartile(n):\n",
        "    def quartile_(x):\n",
        "        return np.percentile(x, n * 25)\n",
        "    quartile_.__name__ = 'q%s' % n\n",
        "    return quartile_\n",
        "\n",
        "stats = df.groupby([\"STATE\", \"DECADE\"]).agg({\"FIRE_SIZE\": [np.min, np.max, np.mean, quartile(1), quartile(3)]}).reset_index()\n",
        "stats.columns = [col[0] if len(col[-1]) == 0 else col[-1] for col in stats.columns.values]\n",
        "stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yySutBmFsaiS"
      },
      "source": [
        "def hex_to_rgb(hex):\n",
        "  return tuple(int(hex.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
        "\n",
        "def rgb_to_hex(rgb):\n",
        "  return '#%02x%02x%02x' % rgb\n",
        "\n",
        "def interpolate_hex_codes(start, end, t):\n",
        "  return rgb_to_hex(interpolate_rgb(hex_to_rgb(start), hex_to_rgb(end), t))\n",
        "\n",
        "def interpolate_rgb(rgb1, rgb2, t):\n",
        "  return tuple([interpolate_int(x, y, t) for x, y in zip(rgb1, rgb2)])\n",
        "\n",
        "def interpolate_int(i1, i2, t):\n",
        "  return round(i1 + ((i2 - i1) * t))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjxkjRtLdFaX"
      },
      "source": [
        "color_range = \"#ffffb2\", \"#b10026\"\n",
        "mid_color = interpolate_hex_codes(color_range[0], color_range[1], 0.5)\n",
        "\n",
        "\"\"\"\n",
        "Definition for Altair Representation of Area Schematized Cartogram\n",
        "\"\"\"\n",
        "\n",
        "decade_radio = alt.binding_radio(options=decades)\n",
        "decade_select = alt.selection_single(fields=['DECADE'], bind=decade_radio, name=\"Decade Select\", init={'DECADE': decades[0]})\n",
        "\n",
        "state_selector = alt.selection_multi(fields=['STATE'])\n",
        "\n",
        "\"\"\" CREATE AREA CARTOGRAM \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "For the absolute life of me I can't figure out if altair will allow dynamic ranges to be set\n",
        "there is absolutely no support for plotting circles with a specificed size\n",
        "this is the closest I have been able to get\n",
        "\"\"\"\n",
        "\n",
        "base = alt.Chart(working_data).encode(\n",
        "  x = alt.X('CIRCLE_X', scale = alt.Scale(domain=[-1, 1]), axis=None),\n",
        "  y = alt.Y('CIRCLE_Y', scale = alt.Scale(domain=[-1, 1]), axis=None),\n",
        ").transform_filter(\n",
        "    decade_select\n",
        ")\n",
        "\n",
        "tooltips = [\n",
        "            alt.Tooltip('STATE', title=\"State\"),\n",
        "            alt.Tooltip('ABBV', title=\"Code\"),\n",
        "            alt.Tooltip('STATE_DECADE_FREQUENCY', title=\"Frequency\"),\n",
        "            alt.Tooltip('CATAGORY_LETTER', title=\"Weighted Fire Class\"),\n",
        "            ]\n",
        "\n",
        "chart = base.mark_circle(stroke='black', strokeWidth=2).encode(\n",
        "  color = alt.condition(state_selector, 'CATAGORY:Q', alt.value('lightgray'), legend=None, scale=alt.Scale(range=color_range)),\n",
        "  size= alt.Size('CIRCLE_A', scale=alt.Scale(domain=[0, 4], range=[0, chart_size * chart_size]), legend=None),\n",
        "  tooltip=tooltips\n",
        ").properties(width=chart_size, height=chart_size)\n",
        "\n",
        "text = base.mark_text().encode(\n",
        "    text='ABBV',\n",
        "    tooltip=tooltips\n",
        ")\n",
        "labeled_chart = ((chart + text)).add_selection(\n",
        "      state_selector\n",
        ").add_selection(\n",
        "    decade_select\n",
        ")\n",
        "\n",
        "legend = alt.Chart(working_data).mark_circle(size=150).encode(\n",
        "      x=alt.X(\n",
        "          \"CATAGORY_LETTER:N\",\n",
        "          axis=alt.Axis(domain=False, ticks=False, labelAngle=0, orient='top'),\n",
        "          title=\"Weighted average fire size classification\"\n",
        "      ),\n",
        "\n",
        "      color=alt.Color('CATAGORY:Q', legend=None, scale=alt.Scale(range=(\"#ffff00\", \"#ff0000\")))\n",
        ")\n",
        "\n",
        "wav_bars = alt.Chart(working_data).mark_bar().encode(\n",
        "    x=alt.X(\"count(CATAGORY_LETTER)\", axis=alt.Axis(title='Overall Occurences')),\n",
        "    y=alt.Y(\"CATAGORY_LETTER:N\", axis=alt.Axis(title='Classification')),\n",
        "    color=alt.Color('CATAGORY_LETTER', scale=alt.Scale(range=[mid_color, mid_color]), legend=None),\n",
        "    tooltip=[alt.Tooltip('CATAGORY_LETTER', title=\"Class\"), alt.Tooltip('count(CATAGORY_LETTER)', title=\"#\")]\n",
        ").transform_filter(\n",
        "    decade_select\n",
        ")\n",
        "\n",
        "causes_count = len(cause_counts[\"STAT_CAUSE_DESCR\"].unique())\n",
        "causes_selection = alt.selection_multi(fields=['STAT_CAUSE_DESCR'])\n",
        "\n",
        "causes_bars = alt.Chart(cause_counts).mark_bar().encode(\n",
        "    x='STATE',\n",
        "    y=alt.Y('COUNT'),\n",
        "    color=alt.Color('STAT_CAUSE_DESCR', legend=None),\n",
        "    tooltip=[alt.Tooltip('STATE', title=\"State\"), alt.Tooltip('COUNT', title=\"#\"), alt.Tooltip('STAT_CAUSE_DESCR', title=\"Cause\")]\n",
        ").transform_filter(\n",
        "    decade_select\n",
        ").transform_filter(\n",
        "    state_selector\n",
        ").transform_filter(\n",
        "    causes_selection\n",
        ").add_selection(\n",
        "    state_selector\n",
        ").add_selection(\n",
        "    causes_selection\n",
        ")\n",
        "\n",
        "causes_legend = alt.Chart(cause_counts).mark_circle().encode(\n",
        "    y=alt.Y('STAT_CAUSE_DESCR:N', axis=alt.Axis(orient='right')),\n",
        "    color=alt.condition(causes_selection, alt.Color('STAT_CAUSE_DESCR:N', legend=None), alt.value('lightgray'))\n",
        ").add_selection(\n",
        "    causes_selection\n",
        ")\n",
        "causes_bars = causes_bars | causes_legend\n",
        "\n",
        "\n",
        "fire_classifications_count = len(sizes_counts[\"FIRE_SIZE_CLASS\"].unique())\n",
        "sizes_bars = alt.Chart(sizes_counts).mark_bar().encode(\n",
        "    x='STATE',\n",
        "    y='COUNT',\n",
        "    color=alt.Color('FIRE_SIZE_CLASS:O', scale=alt.Scale(range=[interpolate_hex_codes(color_range[0], color_range[1], i/(fire_classifications_count-1)) for i in range(fire_classifications_count)], type=\"ordinal\")),\n",
        "    tooltip=[alt.Tooltip('STATE', title=\"State\"), alt.Tooltip('TOTAL', title=\"Total\"),  alt.Tooltip('COUNT', title=\"#\"), alt.Tooltip('FIRE_SIZE_CLASS', title=\"Class\")]\n",
        ").transform_filter(\n",
        "    decade_select\n",
        ").transform_filter(\n",
        "    state_selector\n",
        ").add_selection(\n",
        "    state_selector\n",
        ")\n",
        "\n",
        "outlines = alt.Chart(sizes_counts).mark_bar(filled=False, color='black', strokeWidth=0.5).encode(\n",
        "    x='STATE',\n",
        "    y='TOTAL',\n",
        "    #tooltip=[alt.Tooltip('STATE', title=\"State\"), alt.Tooltip('TOTAL', title=\"Total\"),  alt.Tooltip('COUNT', title=\"#\"), alt.Tooltip('FIRE_SIZE_CLASS', title=\"Class\")]\n",
        ").transform_filter(\n",
        "    decade_select\n",
        ").transform_filter(\n",
        "    state_selector\n",
        ")\n",
        "\n",
        "sizes_bars = outlines + sizes_bars\n",
        "\n",
        "alt.data_transformers.disable_max_rows()\n",
        "whiskers = alt.LayerChart(stats).encode(\n",
        "    x='STATE',\n",
        "    tooltip=['amin:Q', 'q1:Q', 'mean:Q', 'q3:Q', 'amax:Q']\n",
        ").add_layers(\n",
        "    alt.Chart().mark_rule().encode(y=alt.Y('amin:Q', scale=alt.Scale(type='log')), y2='amax:Q'),\n",
        "    alt.Chart().mark_bar(width=15).encode(y='q1:Q', y2='q3:Q'),\n",
        "    alt.Chart().mark_tick(color='white', width=15).encode(y='mean:Q'),\n",
        ").transform_filter(\n",
        "    decade_select\n",
        ").transform_filter(\n",
        "    state_selector\n",
        ")\n",
        "\n",
        "((labeled_chart | (legend & wav_bars & whiskers)) & alt.hconcat(causes_bars, sizes_bars, resolve = alt.Resolve(scale=alt.LegendResolveMap(color=alt.ResolveMode('independent')))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrFNEHNsmAzq"
      },
      "source": [
        "**TESTING GRAVEYARD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b57K0fIMr2X5"
      },
      "source": [
        "model, _ = train_model(df['FIRE_YEAR'].values.reshape(-1,1), df['FIRE_SIZE'].values.reshape(-1,1))\n",
        "plot_model(df['FIRE_YEAR'], df['FIRE_SIZE'], model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_TN7qUuV-ql"
      },
      "source": [
        "X = df['FIRE_YEAR'].values.reshape(-1,1)\n",
        "y = df['FIRE_SIZE'].values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfopqcdFWjAI"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cArys_K2XkSw"
      },
      "source": [
        "#Finding the intercept and slope of the data\n",
        "\n",
        "print(model.intercept_)\n",
        "print(model.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7-PE7tHp9q-"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Comparing with actual data\n",
        "\n",
        "plottableDf = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\n",
        "plottableDf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmpRdef-qU3P"
      },
      "source": [
        "df1 = plottableDf.head(20)\n",
        "df1.plot(kind='bar',figsize=(16,10))\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
        "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMGM8odLrcqL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhWNPga5rr19"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}